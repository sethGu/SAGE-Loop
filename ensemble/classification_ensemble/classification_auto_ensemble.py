import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°
project_root = os.path.abspath(
    os.path.join(os.path.dirname(__file__),   # å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
                 "..", "..")                  # å‘ä¸Šè·³ä¸¤çº§
)
sys.path.append(project_root)

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
from mycaafe import CAAFEClassifier  # Automated Feature Engineering for tabular datasets
from mycaafe import data
from mycaafe.run_llm_code import run_llm_code

import re
import time
import psutil  

from utils.ensembleUtils import getMetaModel_list
from utils.CL_ensemble_utils import get_classification_param_prompt
from utils.model_generate import (
    build_prompt_samples,
    get_model_prompt,
    generate_model_2,
)
from utils.ensemble_utils2 import stacking_ensemble_v2

import copy
import pandas as pd
import torch
import statistics

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
    

import numpy as np
import pickle
import random
import argparse
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=ResourceWarning)


def get_data_split(ds, seed):
    def get_df(X, y):
        df = pd.DataFrame(
            data=np.concatenate([X, np.expand_dims(y, -1)], -1), columns=ds[4]
        )
        cat_features = ds[3]
        for c in cat_features:
            if len(np.unique(df.iloc[:, c])) > 50:
                cat_features.remove(c)
                continue
            df[df.columns[c]] = df[df.columns[c]].astype("int32")
        return df.infer_objects()

    ds = copy.deepcopy(ds)

    X = ds[1].numpy() if type(ds[1]) == torch.Tensor else ds[1]
    y = ds[2].numpy() if type(ds[2]) == torch.Tensor else ds[2]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=seed
    )

    df_train = get_df(X_train, y_train)
    df_test = get_df(X_test, y_test)
    df_train.iloc[:, -1] = df_train.iloc[:, -1].astype("category")
    df_test.iloc[:, -1] = df_test.iloc[:, -1].astype("category")

    return ds, df_train, df_test

def load_origin_data(dataset_name, seed=0):
    # éœ€è¦èµ° .pkl çš„æ—§æ•°æ®é›†å…³é”®è¯ï¼ˆå­ä¸²åŒ¹é…ï¼‰
    old_keys = ('credit','cd1','cc1','ld1','cc2','cd2','cf1','balance-scale')
    name_l = dataset_name.lower()
    is_old = any(k in name_l for k in old_keys)

    if is_old:
        loc = f"{project_root}/data/{dataset_name}.pkl"
        with open(loc, 'rb') as f:
            ds = pickle.load(f)

        # credit ç³»åˆ—éœ€è¦å…ˆ splitï¼Œå…¶å®ƒæ—§æ•°æ®é›†ç›´æ¥ ds[1]/ds[2]
        if 'credit' in name_l:
            ds, df_train, df_test = get_data_split(ds, seed=seed)
        else:
            df_train, df_test = ds[1], ds[2]

        target_column_name = ds[4][-1]
        dataset_description = ds[-1]
        return df_train, df_test, target_column_name, dataset_description

def base_model(seed):
    rforest = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight='balanced')
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)  # å¯é‡å¤çš„éšæœºæ•°æ®åˆ’åˆ†
    param_grid = {
        "min_samples_leaf": [0.001, 0.01, 0.05],  # è°ƒæ•´èŒƒå›´
        "max_depth": [5, 10, None]  # æ–°å¢æ·±åº¦æ§åˆ¶
    }
    gsmodel = GridSearchCV(rforest, param_grid, cv=cv, scoring='f1')

    return gsmodel

def code_exec(code):
    try:
        # å°è¯•ç¼–è¯‘æ£€æŸ¥ï¼ˆcompile æˆ AST å†æ‰§è¡Œï¼‰
        compiled_code = compile(code, "<string>", "exec")
        exec(compiled_code, globals())
        return None
    except Exception as e:
        print("Code could not be executed:", e)
        return str(e)

def print_stats(name, values):
    print(f"{name}: {np.mean(values):.2f} Â± {np.std(values):.2f}")

def clean_llm_code(code: str) -> str:
    import re
    # å»é™¤ ``` å¼€å¤´çš„ä»£ç å—æ ‡è®°å’Œæœ«å°¾é™„åŠ å†…å®¹
    code = re.sub(r"^```python\s*", "", code.strip(), flags=re.IGNORECASE)
    code = re.sub(r"```$", "", code.strip())

    # æ¸…é™¤ <end> å’Œéä»£ç æ–‡å­—ï¼ˆå¯èƒ½æ¥è‡ª LLMï¼‰
    code = re.sub(r"<end>", "", code)

    # ç§»é™¤ LLM è¾“å‡ºä¸­çš„è§£é‡Šæ®µæˆ–æ–‡æœ¬å¼€å¤´é”™è¯¯æç¤º
    lines = code.strip().splitlines()
    cleaned_lines = []
    for line in lines:
        if line.strip().startswith("class myclassifier") or line.strip().startswith("import") or line.strip().startswith(
                "from"):
            cleaned_lines.append(line)
        elif cleaned_lines:  # å¦‚æœå·²å¼€å§‹è®°å½•ä»£ç å—ï¼Œç»§ç»­æ·»åŠ åç»­ä»£ç 
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines)

def to_pd(df_train, target_name):
    y = df_train[target_name].astype(int)
    x = df_train.drop(target_name, axis=1)

    return x, y

def format_mean_std(values: list[float]) -> str:
    if not values:
        return "N/A"
    mean = statistics.mean(values)
    std_dev = statistics.stdev(values)
    return f"{mean:.2f}Â±{std_dev:.2f}"

def generate_feat(
        base_classifier,
        df_train,
        df_test,
        dataset_name,
        round_num = 0,
        llm_model='gpt-3.5-turbo',
        iterations=10,
        target_column_name='class', # ç›®æ ‡åˆ—å
        dataset_description=None,
        task_type="classification",
        base_url:str=None,api_key:str=None
):
    if base_url is None or api_key is None:
        raise ValueError("base_url and api_key must be provided.")
    caafe_clf = CAAFEClassifier(base_classifier=base_classifier,
                                llm_model=llm_model,
                                iterations=iterations,
                               )

    caafe_clf.fit_pandas(df_train,
                         target_column_name=target_column_name,
                         dataset_description=dataset_description,
                         dataset_name=dataset_name,
                         round_num=round_num,
                         task_type=task_type,
                         base_url=base_url,
                         api_key=api_key
                         )

    df_train_aug = run_llm_code(caafe_clf.code, df_train, target_column_name)  # ç‰¹è¯Šå·¥ç¨‹åçš„æ•°æ®é›†è®­ç»ƒé›†
    df_test_aug = run_llm_code(caafe_clf.code, df_test, target_column_name)  # ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®é›†æµ‹è¯•é›†

    df_train_aug = df_train_aug[caafe_clf.final_columns]
    df_test_aug = df_test_aug[caafe_clf.final_columns]

    return df_train_aug, df_test_aug

# --- æ–°å¢: è·å–å½“å‰è¿›ç¨‹RSSå†…å­˜(MB) ---
def get_current_rss_mb():
    pid = os.getpid()
    process = psutil.Process(pid)
    mem_info = process.memory_info()
    return mem_info.rss / 1024 / 1024  # è½¬æ¢ä¸º MB
# -----------------------------------

if __name__ == '__main__':
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument('-g', '--gpus', default="0", type=str, help='GPUè®¾ç½®')
    parser.add_argument('-s', '--default_seed', default=42, type=int, help='éšæœºç§å­')
    parser.add_argument('-l', '--llm', default='gpt-3.5-turbo', type=str, help='å¤§æ¨¡å‹')
    # parser.add_argument('-l', '--llm', default='gpt-4o', type=str, help='å¤§æ¨¡å‹')
    parser.add_argument('-e', '--exam_iterations', default=2, type=int, help='å®éªŒæ¬¡æ•°')  # ä¿®æ”¹ä¸º5æ¬¡å®éªŒ
    parser.add_argument('-f', '--feat_iterations', default=2, type=int, help='ç‰¹å¾è¿­ä»£æ¬¡æ•°')
    parser.add_argument('-m', '--model_iterations', default=3, type=int, help='æ¨¡å‹è¿­ä»£æ¬¡æ•°')
    parser.add_argument('-p', '--param_iterations', default=1, type=int,help='å‚æ•°è°ƒä¼˜æ¬¡æ•°')
    parser.add_argument('-d', '--dataset', default = "cf1",help = "æ•°æ®é›†")
    parser.add_argument('--enable_optimization', action='store_true', default=True, 
                        help='æ˜¯å¦å¯ç”¨æ¨¡å‹è¶…å‚ä¼˜åŒ– True ä»£è¡¨å¯ç”¨ False ä»£è¡¨ç¦ç”¨')
    parser.add_argument('--enable_feedback', action='store_true', default=False,
                        help='æ˜¯å¦å¯ç”¨æ¨¡å‹ç”Ÿæˆåé¦ˆ True ä»£è¡¨å¯ç”¨ False ä»£è¡¨ç¦ç”¨')
    args = parser.parse_args()

    """
    openAI API è®¾ç½® 
    """
    # TODO     openAI API è®¾ç½® 
    # --- è·å–ç¯å¢ƒå˜é‡ ---
    env_url = os.getenv("OPENAI_BASE_URL")
    env_key = os.getenv("OPENAI_API_KEY")
    # --- æ‰‹åŠ¨è®¾ç½® ---
    manual_url = ""
    manual_key = ""

    # --- é€‰æ‹©ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > æ‰‹åŠ¨è®¾ç½® ---
    if env_url and env_key:
        base_url = env_url
        api_key = env_key
    elif manual_url and manual_key:
        base_url = manual_url
        api_key = manual_key
    else:
        raise ValueError(
            "No valid OpenAI API configuration found. "
            "Please set environment variables or manual config."
        )


    # æ¨¡å‹æ ‡ç­¾ï¼Œå…¨å±€åŒºåˆ† LLM ç”Ÿæˆçš„æ¨¡å‹
    model_tab = 1

    ds_name = args.dataset
    # cd1 cc1 ld1  cc2 cd2 cf1 balance-scale ds_credit
    print(f"=========== Dataset {ds_name} ===========")
    # æ‰“å°è¶…å‚ä¼˜åŒ–å¼€å…³çŠ¶æ€
    print(f"è¶…å‚ä¼˜åŒ–çŠ¶æ€: {'å¯ç”¨' if args.enable_optimization else 'ç¦ç”¨'}")
    # æ‰“å°æ¨¡å‹ç”Ÿæˆåé¦ˆå¼€å…³çŠ¶æ€
    print(f"æ¨¡å‹ç”Ÿæˆåé¦ˆçŠ¶æ€: {'å¯ç”¨' if args.enable_feedback else 'ç¦ç”¨'}")

    # ç”¨äºå­˜å‚¨æ¯æ¬¡é›†æˆå­¦ä¹ çš„æŒ‡æ ‡çš„ç»“æœ
    test_acc_list_ensemble = []
    test_f1_list_ensemble = []
    test_auc_list_ensemble = []
    test_pre_list_ensemble = []
    test_rec_list_ensemble = []

    # æ–°å¢ï¼šç”¨äºå­˜å‚¨VotingæŒ‡æ ‡çš„ç»“æœ
    test_acc_list_ensemble_voting = []
    test_f1_list_ensemble_voting = []
    test_auc_list_ensemble_voting = []
    test_pre_list_ensemble_voting = []
    test_rec_list_ensemble_voting = []

    # ---------------------- æ—¶é—´ & å†…å­˜ ç»Ÿè®¡åˆå§‹åŒ– ----------------------
    all_time_start = time.time()
    feat_time_list = []  # å­˜å‚¨æ¯è½®ç‰¹å¾ç”Ÿæˆæ—¶é—´
    token_model_list = [] # å­˜å‚¨æ¯è½®æ¨¡å‹ç”Ÿæˆä»¥åŠä¼˜åŒ– token æ•°
    token_feat_list = [] # å­˜å‚¨æ¯è½®ç‰¹å¾ç”Ÿæˆ token æ•°   
    total_llm_time = 0.0  # å­˜å‚¨LLMè°ƒç”¨+è§£ææ€»æ—¶é—´
    
    global_peak_memory_mb = 0.0  # --- å†…å­˜ç»Ÿè®¡: å…¨å±€å³°å€¼å†…å­˜ ---
    
    # åˆå§‹åŒ–æ£€æŸ¥ä¸€æ¬¡å†…å­˜
    curr_mem = get_current_rss_mb()
    if curr_mem > global_peak_memory_mb:
        global_peak_memory_mb = curr_mem
    # -----------------------------------------------------------

    # å®éªŒæ¬¡æ•°
    for exp in range(args.exam_iterations):
        print(f"=========== Experiment {exp + 1}/{args.exam_iterations} ===========")
        

        total_token_model_exp = 0 # å­˜å‚¨æ¯æ¬¡å®éªŒçš„æ€» token æ•°
        # --- å†…å­˜ç»Ÿè®¡: å®éªŒå¼€å§‹ ---
        curr_mem = get_current_rss_mb()
        if curr_mem > global_peak_memory_mb:
            global_peak_memory_mb = curr_mem
        # ------------------------

        # å­˜å‚¨æ¯æ¬¡å®éªŒç»“æœçš„åˆ—è¡¨
        test_auc_list = []
        seed = args.default_seed + exp
        # è®¾ç½®éšæœºç§å­
        random.seed(seed)
        np.random.seed(seed)
        # åŠ è½½æ•°æ®é›†ã€åŠ è½½æ¨¡å‹
        df_train_aug, df_test_aug, target_column_name, dataset_description = load_origin_data(ds_name)
        baseline_model = base_model(seed)  # éšæœºæ£®æ— model

        
        feat_start = time.time()
        df_train_aug, df_test_aug = generate_feat(
            base_classifier=baseline_model,  # è¯„ä»·ç”Ÿæˆç‰¹å¾çš„æ¨¡å‹æ˜¯ éšæœºæ£®æ—
            df_train=df_train_aug,
            df_test=df_test_aug,
            dataset_name=ds_name,
            round_num=exp + 1,
            llm_model=args.llm,
            iterations=args.feat_iterations,
            target_column_name=target_column_name,
            dataset_description=dataset_description,
            base_url=base_url,
            api_key=api_key

        )
        print("ç‰¹å¾ç”Ÿæˆå®Œæˆ")
        feat_end = time.time()
        feat_elapsed = feat_end - feat_start
        feat_time_list.append(feat_elapsed)
        print(f"ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {feat_elapsed:.2f} ç§’")

        
        
        # --- å†…å­˜ç»Ÿè®¡: ç‰¹å¾ç”Ÿæˆå ---
        curr_mem = get_current_rss_mb()
        if curr_mem > global_peak_memory_mb:
            global_peak_memory_mb = curr_mem
        # -----------------------------------------------------------

        df_train_aug,df_valid_aug = train_test_split(df_train_aug,test_size=0.25,random_state=seed,stratify=df_train_aug[target_column_name])
        

        # æ•°æ®è½¬æ¢ å¾—åˆ°ç‰¹å¾çŸ©é˜µå’Œ æ ‡ç­¾ï¼ˆç›®æ ‡ï¼‰ å‘é‡
        train_aug_x, train_aug_y = to_pd(df_train_aug, target_column_name)
        val_aug_x, val_aug_y = to_pd(df_valid_aug, target_column_name)
        test_aug_x, test_aug_y = to_pd(df_test_aug, target_column_name)

        # æ„é€ æç¤ºè¯éœ€è¦çš„æ•°æ®æ ¼å¼
        s = build_prompt_samples(df_train_aug)

        # LLM ç”Ÿæˆ åˆ†ç±»æ¨¡å‹ä»£ç çš„æç¤ºè¯ prompt
        model_prompt = get_model_prompt(
            target_column_name=target_column_name,
            samples=s,
        )


        model_messages = [
            {
                "role": "system",
                "content": (
                    "You are a top-level machine learning classification expert.\n"
                    "Your task is to help me iteratively search for the most suitable classifier model.\n"
                    "Your primary goal is to maximize the AUC (Area Under the ROC Curve) on the test set.\n"
                    "You must focus on improving AUC more than any other metric.\n"
                    "Your answer should only generate valid Python code.\n\n"
                    "**IMPORTANT: You MUST utilizing all available CPU cores for training to speed up the process.**\n"
                    "**- For scikit-learn compatible models (RandomForest, etc.) and XGBoost/LightGBM, ALWAYS set `n_jobs=-1`.**\n"
                    "**- For CatBoost, ALWAYS set `thread_count=-1`.**\n"
                    "**- Always explicitely set this parameter in the model initialization.**"
                ),
            },
            {
                "role": "user",
                "content": model_prompt,  # ä¿æŒç”Ÿæˆæ¨¡å‹çš„å…·ä½“æç¤ºç»“æ„ä¸å˜
            },
        ]


        model_iter = args.model_iterations
        best_auc = 0
        best_code = None
        i = 0

        # å‚ä¸é›†æˆå­¦ä¹ åŸºç¡€æ¨¡å‹åˆ—è¡¨
        base_models = []
        # æ¨¡å‹ç”Ÿæˆè¿­ä»£
        while i < model_iter:
            try:
                # ---------------------- LLMæ¨¡å‹ç”Ÿæˆæ—¶é—´ç»Ÿè®¡ ----------------------
                model_llm_start = time.time()
                # ç”Ÿæˆä¸‹æ¸¸æ¨¡å‹ä»£ç 
                res = generate_model_2(args.llm, model_messages,base_url,api_key)
                code = res['code']
                total_tokens_model = res['total_tokens']
                total_token_model_exp += total_tokens_model
                # todo åŠ  code_clean ä»£ç 
                code = clean_llm_code(code)
                model_llm_end = time.time()
                total_llm_time += (model_llm_end - model_llm_start)
                # -----------------------------------------------------------

                # åŠ¨æ€ä¿®æ”¹ç±»å
                new_class_name = f"myclassifier_{i + 1}"
                code = re.sub(r'class\s+myclassifier\w*\s*:', f'class {new_class_name}:', code)
                print(f"----------------------------åŸå§‹ä»£ç -----------------------")
                print(code)
            except Exception as e:
                print("Error in LLM API." + str(e))
                continue

            e = code_exec(code)
            # æ£€æŸ¥ç¼–è¯‘é”™è¯¯
            if e is not None:  # ç”Ÿæˆçš„ä»£ç æ‰§è¡Œå‡ºé”™ å°†é”™è¯¯ä¿¡æ¯åé¦ˆç»™LLMä»¥ç”Ÿæˆä¿®å¤åçš„ä»£ç 
                model_messages += [
                    {"role": "assistant", "content": code},
                    {
                        "role": "user",
                        "content": f"""
                            The classifier code execution failed with error: {type(e)} {e}.\n Code: ```python{code}```
                            Remember, your answer should only generate code.
                            Do not include explanations or comments outside the code block.
                            Generate next code block(fixing error?):
                            """,
                    },
                ]
                continue


            try:
                # æ¨¡å‹å®ä¾‹
                model_class = globals()[new_class_name]
                model = model_class()
                model_copy = copy.deepcopy(model)
                model.fit(train_aug_x, train_aug_y)
                pred = model.predict(val_aug_x)
                proba = model.predict_proba(val_aug_x)
                model_list_append = model_copy
                
                # --- å†…å­˜ç»Ÿè®¡: æ¨¡å‹è®­ç»ƒå ---
                curr_mem = get_current_rss_mb()
                if curr_mem > global_peak_memory_mb:
                    global_peak_memory_mb = curr_mem
                # -------------------------

            except Exception as e:
                print("Model code execution failed with error:" + str(e))
                model_messages += [
                    {"role": "assistant", "content": code},
                    {
                        "role": "user",
                        "content": f"""
                        Code execution failed with error: {type(e)} {e}.\n Code: ```python{code}```\n Generate next code block(fixing error?):
                        """,
                    },
                ]
                continue


            test_auc = roc_auc_score(val_aug_y, proba) * 100

            # todo åœ¨è¿™åé¢åŠ å…¥å‚æ•°ä¼˜åŒ–
            param_best_code = code
            param_best_auc = test_auc
            
            # å¦‚æœå¯ç”¨è¶…å‚ä¼˜åŒ–ï¼Œæ‰åˆå§‹åŒ–å‚æ•°ä¼˜åŒ–æç¤ºè¯
            if args.enable_optimization:
                param_prompt = get_classification_param_prompt(
                    best_code=param_best_code,
                    best_auc=param_best_auc,
                    dataset_description=dataset_description,
                    X_test=val_aug_x,
                    feature_columns=train_aug_x.columns.tolist(),
                    dataset_name=ds_name,
                    max_rows=10
                )

                param_messages = [
                    {
                        "role": "system",
                        "content": (
                            "You are a classification optimization assistant.\n"
                            "Your task is to help me improve the test AUC of the given classifier\n"
                            "by tuning hyperparameters only. Your answer must contain only executable Python code.\n\n"
                            "**PRIMARY GOAL:** Focus on finding better *combinations* of hyperparameters, not just isolated changes to single parameters.\n\n"
                            "**CRITICAL CONSTRAINT 1: Parallelism**\n"
                            "- **You MUST preserve or add `n_jobs=-1` (or `thread_count=-1` for CatBoost) to ensure multi-core training.**\n"
                            "- Do NOT remove this parameter during optimization.\n\n"
                            "**CRITICAL CONSTRAINT 2: Efficient Search Space**\n"
                            "For ensemble/tree-based models (e.g., RandomForest, XGBoost, LightGBM, CatBoost):\n"
                            "- **n_estimators (or equivalent):** MUST be $\le 300$ (e.g., 50-300).\n"
                            "- **max_depth (or equivalent):** MUST be $\le 10$ (e.g., 3-10).\n"
                            "- **num_leaves (for LightGBM):** MUST be $\le 64$.\n"
                            "For other models (e.g., SVM, Neural Networks, kNN):\n"
                            "- **C/alpha (regularization):** Use standard ranges (e.g., 0.001 to 10.0).\n"
                            "- **max_iter (or equivalent):** MUST be $\le 500$.\n"
                            "Prioritize small, efficient parameter values."
                        )
                    },
                    {
                        "role": "user",
                        "content": param_prompt
                    },
                ]
                # -----------------------------------------------------------------------------

            # å¼€å§‹å‚æ•°ä¼˜åŒ–è¿­ä»£ï¼Œåªæœ‰å¯ç”¨ä¼˜åŒ–æ—¶æ‰æ‰§è¡Œ
            param_iter_range = range(args.param_iterations) if args.enable_optimization else range(0)
            for p_iter in param_iter_range:
                print(f"++++++ ç¬¬ {p_iter + 1} æ¬¡ä¼˜åŒ– +++++++")
                try:
                    # ---------------------- LLMå‚æ•°ä¼˜åŒ–æ—¶é—´ç»Ÿè®¡ ----------------------
                    param_llm_start = time.time()
                    res = generate_model_2(args.llm, param_messages,base_url,api_key)
                    param_code = res['code']
                    token_parm = res['total_tokens']
                    total_token_model_exp += token_parm

                    param_code = clean_llm_code(param_code)
                    param_llm_end = time.time()
                    total_llm_time += (param_llm_end - param_llm_start)
                    # -----------------------------------------------------------

                    param_new_class_name = f"myclassifier_{i+1}_param_{p_iter + 1}"
                    param_code = param_code.replace(f"class myclassifier_{i+1}:", f"class {param_new_class_name}:")
                    param_code = param_code.replace(f"class myclassifier_{i+1}_param_{p_iter}:",f"class {param_new_class_name}:")

                    param_err = code_exec(param_code)

                    if param_err is not None:
                        param_messages += [
                            {"role": "assistant", "content": param_code},
                            {"role": "user", "content": "Code failed. Fix it and regenerate."},
                        ]
                        continue

                    print('---------------ä¼˜åŒ–åçš„ä»£ç \n' + param_code)
                    # ä½¿ç”¨æ–°çš„ç±»ååˆ›å»ºæ¨¡å‹
                    model_class = globals()[param_new_class_name]
                    myclassifier_tuned = model_class()
                    model_copy = copy.deepcopy(myclassifier_tuned)
                    myclassifier_tuned.fit(train_aug_x, train_aug_y)
                    proba_tuned = myclassifier_tuned.predict_proba(val_aug_x)
                    auc_tuned = roc_auc_score(val_aug_y, proba_tuned) * 100

                    if auc_tuned > param_best_auc:
                        print(f"å‚æ•°ä¼˜åŒ–æ•ˆæœæå‡ï¼š{param_best_auc} --> {auc_tuned}")
                        param_best_auc = auc_tuned
                        param_best_code = param_code
                        model_list_append = model_copy
                    
                    # --- å†…å­˜ç»Ÿè®¡: ä¼˜åŒ–æ¨¡å‹è®­ç»ƒå ---
                    curr_mem = get_current_rss_mb()
                    if curr_mem > global_peak_memory_mb:
                        global_peak_memory_mb = curr_mem
                    # -----------------------------


                    param_messages += [
                        {"role": "assistant", "content": param_code},
                        {"role": "user",
                            "content": f"Current AUC: {auc_tuned:.2f}, Best AUC: {param_best_auc:.2f}. Please improve further."},
                    ]

                except Exception as e:
                    print("Tuning failed:", str(e))
                    continue

            # æ›´æ–°å…¨å±€æœ€ä½³æ¨¡å‹å’Œå…¨å±€æœ€ä½³å‚æ•°
            if best_auc < param_best_auc:
                best_auc = param_best_auc
                best_code = param_best_code

            # åŠ å…¥è°ƒå‚åæ¨¡å‹
            base_models.append(model_list_append)

            # å­˜å‚¨ç»“æœ
            test_auc_list.append(param_best_auc)

            # æ‰“å°å½“å‰å®éªŒè¯¦ç»†ç»“æœ
            print(f"å½“å‰å®éªŒç»“æœç¬¬ {i+1}/{args.model_iterations}")
            print(f"Test  AUC: {param_best_auc:.2f}")
            # while å¾ªç¯ç»§ç»­
            i = i + 1

            # ä¸‹ä¸€è½®æ¨¡å‹ç”Ÿæˆæç¤ºè¯æ‹¼æ¥ - ä»…å½“å¯ç”¨åé¦ˆæ—¶æ‰æ·»åŠ 
            if args.enable_feedback and len(code) > 10:
                model_messages += [
                    {"role": "assistant", "content": param_best_code},
                    {
                        "role": "user",
                        "content": f"""
                        âœ… The classifier code executed successfully.

                        ğŸ“ˆ Current model AUC: {param_best_auc:.4f}
                        ğŸ† Best historical AUC so far: {best_auc:.4f}

                        Please now propose a new classifier that is **more likely to improve the AUC** on the given test data.
                        The model must differ from all previous ones **by model type or internal structure**.

                        âš ï¸ Remember:
                        - You must only output valid Python code for a complete classifier named `myclassifier`.
                        - The class must include all imports and implement: `fit`, `predict`, and `predict_proba`.
                        - Do not repeat models you've already used.
                        - Prioritize models that provide reliable probabilistic outputs to help improve AUC.

                        ğŸ¯ Next code block:
                        """,
                    },
                ]
            else:
                model_messages += [
                    {"role": "assistant", "content": param_best_code},
                    {
                        "role": "user",
                        "content": f"""
                        âœ… The classifier code executed successfully.

                        Please now propose a new classifier that is **more likely to improve the AUC** on the given test data.
                        The model must differ from all previous ones **by model type or internal structure**.

                        âš ï¸ Remember:
                        - You must only output valid Python code for a complete classifier named `myclassifier`.
                        - The class must include all imports and implement: `fit`, `predict`, and `predict_proba`.
                        - Do not repeat models you've already used.
                        - Prioritize models that provide reliable probabilistic outputs to help improve AUC.

                        ğŸ¯ Next code block:
                        """,
                    },
                ]
        token_model_list.append(total_token_model_exp)



        """
        é›†æˆå­¦ä¹ 
        """
        # æƒ³è¦æµ‹è¯•çš„å…ƒæ¨¡å‹åå­—åˆ—è¡¨
        metaModelName_list = [
            # 'RandomForestClassifier',
            # 'XGBClassifier',
            # 'LGBMClassifier',
            # 'CatBoostClassifier',
            # 'SVC',
            # 'DecisionTreeClassifier',
            'LogisticRegression',
            # 'BaggingClassifier',
        ]
        # æƒ³è¦æµ‹è¯•çš„å…ƒæ¨¡å‹åˆ—è¡¨
        metaModelName_list = getMetaModel_list(metaModelName_list)

        for meta_model in metaModelName_list:
            # è°ƒç”¨é›†æˆå­¦ä¹ æ–¹æ³•
            result = stacking_ensemble_v2(base_models,train_aug_x,train_aug_y,test_aug_x,test_aug_y)
            stacking_metrics = result['stacking_metrics']

            print(f"\n=========== ç¬¬{exp + 1}æ¬¡Stackingé›†æˆç»“æœ--{type(meta_model).__name__} ===========")
            print(f"Accuracy : {stacking_metrics['accuracy']:.4f}")
            print(f"F1 Score : {stacking_metrics['f1']:.4f}")
            print(f"AUC      : {stacking_metrics['roc_auc']:.4f}")
            print(f"Precision: {stacking_metrics['precision']:.4f}")
            print(f"Recall   : {stacking_metrics['recall']:.4f}")
            # ä¿å­˜é›†æˆç»“æœ
            test_acc_list_ensemble.append(round(stacking_metrics['accuracy'], 5)*100)
            test_f1_list_ensemble.append(round(stacking_metrics['f1'], 5)*100)
            test_pre_list_ensemble.append(round(stacking_metrics['precision'], 5)*100)
            test_rec_list_ensemble.append(round(stacking_metrics['recall'], 5)*100)
            test_auc_list_ensemble.append(round(stacking_metrics['roc_auc'], 5)*100)
            
            # --- å†…å­˜ç»Ÿè®¡: é›†æˆå­¦ä¹ å ---
            curr_mem = get_current_rss_mb()
            if curr_mem > global_peak_memory_mb:
                global_peak_memory_mb = curr_mem
            # -------------------------

            
        # """
        # æ–°å¢:Voting é›†æˆ
        # ç›´æ¥å¯¹ä¸æœ¬è½®ç”Ÿæˆ/è°ƒå‚åçš„ base_models è¿›è¡ŒæŠ•ç¥¨èåˆï¼Œå¹¶è¯„ä¼°æŒ‡æ ‡
        # """
        # voting_result = voting_ensemble(base_models, test_aug_x, test_aug_y)
        # voting_metrics = voting_result['metrics']

        # print(f"\n=========== ç¬¬{exp + 1}æ¬¡ Voting é›†æˆç»“æœ ===========")
        # print(f"Accuracy : {voting_metrics['accuracy']:.4f}")
        # print(f"F1 Score : {voting_metrics['f1']:.4f}")
        # print(f"AUC      : {voting_metrics['auc']:.4f}")
        # print(f"Precision: {voting_metrics['precision']:.4f}")
        # print(f"Recall   : {voting_metrics['recall']:.4f}")

        # # ä¿å­˜Votingé›†æˆç»“æœ
        # test_acc_list_ensemble_voting.append(round(voting_metrics['accuracy'], 5)*100)
        # test_f1_list_ensemble_voting.append(round(voting_metrics['f1'], 5)*100)
        # test_pre_list_ensemble_voting.append(round(voting_metrics['precision'], 5)*100)
        # test_rec_list_ensemble_voting.append(round(voting_metrics['recall'], 5)*100)
        # test_auc_list_ensemble_voting.append(round(voting_metrics['auc'], 5)*100)

    # å®éªŒè¿­ä»£ç»“æŸï¼Œç»Ÿè®¡é›†æˆå­¦ä¹ çš„ç»“æœ
    print(f"\n=========== å®éªŒç»“æŸï¼Œå…±{args.exam_iterations}æ¬¡é›†æˆå­¦ä¹ ç»Ÿè®¡ç»“æœä¿¡æ¯ ===========")
    print("\nStacking é›†æˆå­¦ä¹ ç»“æœ:")
    print('acc: ' + format_mean_std(test_acc_list_ensemble))
    print('f1: ' + format_mean_std(test_f1_list_ensemble))
    print('auc: ' + format_mean_std(test_auc_list_ensemble))
    print('pre: ' + format_mean_std(test_pre_list_ensemble))
    print('rec: ' + format_mean_std(test_rec_list_ensemble))

    all_time_end = time.time()
    all_time = all_time_end - all_time_start
    print(f"æ‰€æœ‰å®éªŒè€—æ—¶: {all_time:.2f} ç§’")
    # print("\nVoting é›†æˆå­¦ä¹ ç»“æœ:")
    # print('voting_acc: ' + format_mean_std(test_acc_list_ensemble_voting))
    # print('voting_f1: ' + format_mean_std(test_f1_list_ensemble_voting))
    # print('voting_auc: ' + format_mean_std(test_auc_list_ensemble_voting))
    # print('voting_pre: ' + format_mean_std(test_pre_list_ensemble_voting))
    # print('voting_rec: ' + format_mean_std(test_rec_list_ensemble_voting))

    # ---------------------- æ—¶é—´ & å†…å­˜ ç»Ÿè®¡è®¡ç®—ä¸å†™å…¥TXT ----------------------
    # 1. æ€»å®éªŒæ—¶é—´ï¼ˆ5è½®ï¼‰
    total_experiment_time = all_time
    # 2. æ¯è½®ç‰¹å¾ç”Ÿæˆå¹³å‡æ—¶é—´
    avg_feat_time = sum(feat_time_list) / len(feat_time_list) if feat_time_list else 0.0
    # 3. é™¤å»ç‰¹å¾ç”Ÿæˆçš„æ€»æ—¶é—´
    total_non_feat_time = total_experiment_time - sum(feat_time_list)
    # 4. LLMè°ƒç”¨+è§£ææ€»æ—¶é—´
    llm_total_time = total_llm_time
    # 5. å‰©ä½™æ­¥éª¤æ—¶é—´ï¼ˆéç‰¹å¾ç”Ÿæˆã€éLLMçš„æ—¶é—´ï¼‰
    remaining_time = total_non_feat_time - llm_total_time

    # æ„é€ ç»Ÿè®¡å†…å®¹ (å¢åŠ å†…å­˜ç»Ÿè®¡)
    time_stats_content = f"""
=========== Dataset {ds_name} ===========
è¶…å‚ä¼˜åŒ–çŠ¶æ€: {'å¯ç”¨' if args.enable_optimization else 'ç¦ç”¨'}
æ¨¡å‹ç”Ÿæˆåé¦ˆçŠ¶æ€: {'å¯ç”¨' if args.enable_feedback else 'ç¦ç”¨'}
ç»¼åˆæ€§èƒ½ç»Ÿè®¡æŠ¥å‘Šï¼ˆ{args.exam_iterations}è½®å®éªŒï¼‰
=========================================
[æ—¶é—´ç»Ÿè®¡]
1. 5è½®å®éªŒæ€»è¿è¡Œæ—¶é—´: {total_experiment_time:.2f} ç§’
2. æ¯è½®å®éªŒç‰¹å¾ç”Ÿæˆæ¨¡å—å¹³å‡æ—¶é—´: {avg_feat_time:.2f} ç§’
3. LLMè°ƒç”¨åŠè§£ææ€»æ—¶é—´ï¼ˆæ¨¡å‹ç”Ÿæˆ+è¶…å‚æ•°ä¼˜åŒ–ï¼‰: {llm_total_time:.2f} ç§’
4. é™¤å»ç‰¹å¾ç”Ÿæˆå’ŒLLMæ—¶é—´åçš„å‰©ä½™æ­¥éª¤æ€»æ—¶é—´: {remaining_time:.2f} ç§’
-----------------------------------------
[å†…å­˜ç»Ÿè®¡]
5. å…¨å±€å³°å€¼å†…å­˜ä½¿ç”¨ (Global Peak Memory): {global_peak_memory_mb:.2f} MB
   (åŸºäºPythonè¿›ç¨‹çš„RSSå¸¸é©»å†…å­˜é›†)
-----------------------------------------
[æ¨¡å‹ç”Ÿæˆå’Œä¼˜åŒ–çš„ token ç»Ÿè®¡]
6. æ¯è½®å®éªŒæ¨¡å‹ç”Ÿæˆå’Œä¼˜åŒ– token æ•°åˆ—è¡¨: {token_model_list}
 --æ‰€æœ‰å®éªŒæ€» token æ•°: {sum(token_model_list)}
 --æ¯è½®å®éªŒå¹³å‡ token æ•°: {sum(token_model_list) / len(token_model_list):.2f}
-----------------------------------------
å„è½®ç‰¹å¾ç”Ÿæˆæ—¶é—´è¯¦æƒ…:
{[f"ç¬¬{i+1}è½®: {t:.2f}ç§’" for i, t in enumerate(feat_time_list)]}
    """
    print(time_stats_content)
    # # å†™å…¥TXTæ–‡ä»¶ï¼ˆè·¯å¾„ç•™ç©ºï¼Œé»˜è®¤å½“å‰ç›®å½•ï¼‰
    # file_path = ""  # å¯è‡ªè¡Œå¡«å†™å…·ä½“è·¯å¾„ï¼Œå¦‚ "./time_stats.txt"
    # if not file_path:
    #     file_path = f"./result/{ds_name}_statistics.txt"  # é»˜è®¤è·¯å¾„

    # try:
    #     with open(file_path, 'w', encoding='utf-8') as f:
    #         f.write(time_stats_content)
    #     print(f"\næ—¶é—´ä¸å†…å­˜ç»Ÿè®¡ç»“æœå·²å†™å…¥: {file_path}")
    # except Exception as e:
    #     print(f"\nå†™å…¥ç»Ÿè®¡æ–‡ä»¶å¤±è´¥: {e}")
