from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
import numpy as np
from sklearn.preprocessing import label_binarize
from sklearn.exceptions import NotFittedError



def stacking_ensemble_multiclass(base_models, meta_model, X_train, y_train, X_test, y_test, n_folds=5, verbose=True,
                                 random_state=42):
    """
    å¤šåˆ†ç±» Stacking é›†æˆå­¦ä¹ å™¨ï¼Œè‡ªåŠ¨è·³è¿‡ä¸æ”¯æŒ predict_proba çš„æ¨¡å‹ã€‚

    å‚æ•°ï¼š
        base_models: listï¼Œä¸€çº§æ¨¡å‹åˆ—è¡¨ï¼ˆå·²å®ä¾‹åŒ–ï¼‰
        meta_model: sklearn æ¨¡å‹ï¼ŒäºŒçº§èåˆæ¨¡å‹ï¼ˆå·²å®ä¾‹åŒ–ï¼‰
        X_train: DataFrameï¼Œè®­ç»ƒç‰¹å¾
        y_train: Seriesï¼Œè®­ç»ƒæ ‡ç­¾
        X_test: DataFrameï¼Œæµ‹è¯•ç‰¹å¾
        y_test: Seriesï¼Œæµ‹è¯•æ ‡ç­¾
        n_folds: intï¼Œäº¤å‰éªŒè¯æŠ˜æ•°
        verbose: boolï¼Œæ˜¯å¦æ‰“å°è¿›åº¦
        random_state: intï¼Œéšæœºç§å­

    è¿”å›ï¼š
        dictï¼ŒåŒ…æ‹¬ï¼š
            - 'meta_model': è®­ç»ƒå¥½çš„èåˆæ¨¡å‹
            - 'preds': æœ€ç»ˆé¢„æµ‹æ ‡ç­¾
            - 'probas': æœ€ç»ˆé¢„æµ‹æ¦‚ç‡
            - 'metrics': å¤šåˆ†ç±»è¯„ä¼°æŒ‡æ ‡
    """

    classes = np.unique(y_train)
    n_classes = len(classes)
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)

    usable_models = []
    usable_model_names = []

    meta_features_train_list = []

    for model_idx, model in enumerate(base_models):
        model_name = type(model).__name__
        if verbose:
            print(f"\nğŸ“š Training base model {model_idx + 1}/{len(base_models)} ({model_name})")

        meta_feat = np.zeros((X_train.shape[0], n_classes))
        skip_model = False

        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
            X_fold_train = X_train.iloc[train_idx]
            y_fold_train = y_train.iloc[train_idx]
            X_fold_val = X_train.iloc[val_idx]

            try:
                model.fit(X_fold_train, y_fold_train)
                val_proba = model.predict_proba(X_fold_val)
                meta_feat[val_idx, :] = val_proba
            except (AttributeError, NotFittedError, ValueError) as e:
                print(f"âš ï¸ Skipping model {model_name}: {e}")
                skip_model = True
                break

        if not skip_model:
            usable_models.append(model)
            usable_model_names.append(model_name)
            meta_features_train_list.append(meta_feat)

    if len(usable_models) == 0:
        raise ValueError("âŒ No base model supports `predict_proba`. Please check your base model list.")

    # æ‹¼æ¥ä¸€çº§æ¨¡å‹è¾“å‡ºä½œä¸ºäºŒçº§è®­ç»ƒç‰¹å¾
    meta_features_train = np.concatenate(meta_features_train_list, axis=1)

    if verbose:
        print("\nğŸ”§ Training meta model...")
    meta_model.fit(meta_features_train, y_train)

    # æµ‹è¯•é›†ä¸€çº§æ¨¡å‹è¾“å‡º


    meta_features_test_list = []
    for model_idx, model in enumerate(usable_models):
        model_name = usable_model_names[model_idx]
        try:
            model.fit(X_train, y_train)
            test_proba = model.predict_proba(X_test)
            meta_features_test_list.append(test_proba)
        except Exception as e:
            print(f"âš ï¸ Test-time model {model_name} failed: {e}")
            raise

    meta_features_test = np.concatenate(meta_features_test_list, axis=1)

    # äºŒçº§æ¨¡å‹é¢„æµ‹
    final_preds = meta_model.predict(meta_features_test)
    final_probas = meta_model.predict_proba(meta_features_test)

    # æ ‡ç­¾ binarize ç”¨äºå¤šç±» AUC
    y_test_binarized = label_binarize(y_test, classes=classes)

    # å¤šåˆ†ç±»è¯„ä¼°
    metrics = {
        "accuracy": accuracy_score(y_test, final_preds),
        "f1": f1_score(y_test, final_preds, average='macro'),
        "precision": precision_score(y_test, final_preds, average='macro'),
        "recall": recall_score(y_test, final_preds, average='macro'),
        "auc": roc_auc_score(y_test_binarized, final_probas, multi_class='ovr', average='macro')
    }

    if verbose:
        print("\nâœ… Evaluation Metrics:")
        for k, v in metrics.items():
            print(f"{k}: {v:.4f}")

    return {
        "meta_model": meta_model,
        "preds": final_preds,
        "probas": final_probas,
        "metrics": metrics
    }



def voting_ensemble(fitted_models, test_x, test_y=None, threshold=0.5):
    """
    å¤šæ¨¡å‹è½¯æŠ•ç¥¨é¢„æµ‹ä¸è¯„ä¼°ã€‚
    æ‰€æœ‰æ¨¡å‹çš„ predict_proba å¿…é¡»è¿”å›ä¸€ç»´æ­£ç±»æ¦‚ç‡ã€‚
    """
    if not fitted_models:
        raise ValueError("æ¨¡å‹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

    probas = []
    for i, model in enumerate(fitted_models):
        if not hasattr(model, "predict_proba"):
            raise AttributeError(f"æ¨¡å‹ {i} ä¸æ”¯æŒ predict_proba()")
        proba = model.predict_proba(test_x)

        if proba.ndim != 1:
            raise ValueError(f"æ¨¡å‹ {i} çš„è¾“å‡ºä¸æ˜¯ä¸€ç»´æ­£ç±»æ¦‚ç‡ï¼Œå®é™… shape: {proba.shape}")
        if np.any(np.isnan(proba)):
            raise ValueError(f"æ¨¡å‹ {i} è¾“å‡ºä¸­å­˜åœ¨ NaN")
        probas.append(proba)

    avg_proba = np.mean(probas, axis=0)

    pred = (avg_proba >= threshold).astype(int)

    metrics = {}
    if test_y is not None:
        acc = accuracy_score(test_y, pred)
        per = precision_score(test_y, pred, zero_division=0)
        rec = recall_score(test_y, pred)
        f1 = f1_score(test_y, pred)
        auc = roc_auc_score(test_y, avg_proba)
        metrics = {
            "accuracy": acc,
            "f1": f1,
            "precision": per,
            "recall": rec,
            "auc": auc
        }

    return {
        "metrics": metrics,
        "proba": avg_proba,
        "pred": pred
    }

def multiclass_voting_ensemble(fitted_models, test_x, test_y=None, average='macro'):
    """
    å¤šåˆ†ç±»ä»»åŠ¡çš„ Soft Voting é›†æˆæ–¹æ³•ã€‚

    å‚æ•°ï¼š
        fitted_models: listï¼Œå·²ç»è®­ç»ƒå¥½çš„æ”¯æŒ predict_proba çš„æ¨¡å‹
        test_x: array-likeï¼Œæµ‹è¯•ç‰¹å¾
        test_y: array-likeï¼Œæµ‹è¯•æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        average: strï¼Œç”¨äºå¤šåˆ†ç±»è¯„ä¼°æŒ‡æ ‡çš„å¹³å‡æ–¹å¼ï¼ˆé»˜è®¤'macro'ï¼‰

    è¿”å›ï¼š
        dictï¼ŒåŒ…æ‹¬é¢„æµ‹æ ‡ç­¾ã€å¹³å‡æ¦‚ç‡ä»¥åŠå¤šåˆ†ç±»è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚ test_y æä¾›ï¼‰
    """
    if not fitted_models:
        raise ValueError("æ¨¡å‹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

    probas = []
    for i, model in enumerate(fitted_models):
        if not hasattr(model, "predict_proba"):
            raise AttributeError(f"æ¨¡å‹ {i} ä¸æ”¯æŒ predict_proba()")
        proba = model.predict_proba(test_x)

        if proba.ndim != 2:
            raise ValueError(f"æ¨¡å‹ {i} çš„ predict_proba è¾“å‡ºåº”ä¸ºäºŒç»´ï¼Œå®é™…ä¸º shape={proba.shape}")
        if np.any(np.isnan(proba)):
            raise ValueError(f"æ¨¡å‹ {i} è¾“å‡ºä¸­å­˜åœ¨ NaN")
        probas.append(proba)

    avg_proba = np.mean(probas, axis=0)
    pred = np.argmax(avg_proba, axis=1)

    metrics = {}
    if test_y is not None:
        classes = np.unique(test_y)
        y_test_bin = label_binarize(test_y, classes=classes)
        metrics = {
            "accuracy": accuracy_score(test_y, pred),
            "f1": f1_score(test_y, pred, average=average),
            "precision": precision_score(test_y, pred, average=average),
            "recall": recall_score(test_y, pred, average=average),
            "auc": roc_auc_score(y_test_bin, avg_proba, multi_class='ovr', average=average)
        }

    return {
        "metrics": metrics,
        "proba": avg_proba,
        "pred": pred
    }


def get_classification_param_prompt(
    best_code, best_auc, dataset_description,
    X_test, feature_columns, dataset_name=None, max_rows=10
):
    """
    ç”Ÿæˆé€‚ç”¨äº LLM åˆ†ç±»æ¨¡å‹å‚æ•°ä¼˜åŒ–çš„æç¤ºè¯
    """
    import pandas as pd
    if isinstance(X_test, pd.DataFrame):
        df_show = X_test.copy()
    else:
        df_show = pd.DataFrame(X_test, columns=feature_columns)

    table = df_show.head(max_rows).to_string(index=False)
    data_shape = df_show.shape if hasattr(df_show, 'shape') else (len(df_show), len(feature_columns))
    if dataset_name is None:
        dataset_name = "unknown"

    prompt = (
        f"Here is the best classification model code so far, with its current AUC score on the test set:\n\n"
        f"Current best AUC: {best_auc:.4f}\n\n"
        f"Model code:\n"
        f"```python\n{best_code}\n```\n"
        f"The downstream classification task is based on the following dataset.\n"
        f"Dataset name: {dataset_name}\n"
        f"Dataset description:\n{dataset_description}\n\n"
        # f"Feature names:\n{', '.join(feature_columns)}\n\n"
        # f"Test set shape: {data_shape}\n"
        # f"Here are the first {max_rows} rows of the test set:\n{table}\n\n"
        "Please ONLY optimize the hyperparameters\n"
        "in the given classification model code to further improve the AUC value.\n"
        "DO NOT change the algorithm type or model structure.\n"
        "Output only a new optimized Python code block.\n"
        "No explanation, only code!\n"
        "IMPORTANT CONTEXT:\n"
        "You are writing classification model code in Python using scikit-learn version 1.6.1.\n"
        "STRICT REQUIREMENT:\n"
        "ONLY use parameters that are supported by scikit-learn version 1.6.1.\n"
        "DO NOT use any parameters that are deprecated or only available in versions prior to 1.2.\n"
        "Refer ONLY to the scikit-learn 1.6.1 documentation for valid parameters and their default values."
    )

    return prompt
