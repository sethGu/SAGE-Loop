import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°
project_root = os.path.abspath(
    os.path.join(os.path.dirname(__file__),   # å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
                 "..", "..")                  # å‘ä¸Šè·³ä¸¤çº§
)
sys.path.append(project_root)



import re

from utils.ensembleUtils import getMetaModel_list
from utils.utils import format_mean_std
from utils.CL_ensemble_utils import stacking_ensemble,get_classification_param_prompt,voting_ensemble
from utils.model_generate import (
    build_prompt_samples,
    get_model_prompt,
    generate_model,
)

import copy
import pandas as pd
import torch

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    roc_auc_score,
    f1_score,
    precision_score,
    recall_score,
)

import numpy as np
import pickle
import random
import argparse
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=ResourceWarning)


def get_data_split(ds, seed):
    def get_df(X, y):
        df = pd.DataFrame(
            data=np.concatenate([X, np.expand_dims(y, -1)], -1), columns=ds[4]
        )
        cat_features = ds[3]
        for c in cat_features:
            if len(np.unique(df.iloc[:, c])) > 50:
                cat_features.remove(c)
                continue
            df[df.columns[c]] = df[df.columns[c]].astype("int32")
        return df.infer_objects()

    ds = copy.deepcopy(ds)

    X = ds[1].numpy() if type(ds[1]) == torch.Tensor else ds[1]
    y = ds[2].numpy() if type(ds[2]) == torch.Tensor else ds[2]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=seed
    )

    df_train = get_df(X_train, y_train)
    df_test = get_df(X_test, y_test)
    df_train.iloc[:, -1] = df_train.iloc[:, -1].astype("category")
    df_test.iloc[:, -1] = df_test.iloc[:, -1].astype("category")

    return ds, df_train, df_test

def load_origin_data(dataset_name, seed=0):
    # éœ€è¦èµ° .pkl çš„æ—§æ•°æ®é›†å…³é”®è¯ï¼ˆå­ä¸²åŒ¹é…ï¼‰
    old_keys = ('credit','cd1','cc1','ld1','cc2','cd2','cf1','balance-scale')
    name_l = dataset_name.lower()
    is_old = any(k in name_l for k in old_keys)

    if is_old:
        loc = f"{project_root}/data/{dataset_name}.pkl"
        with open(loc, 'rb') as f:
            ds = pickle.load(f)

        # credit ç³»åˆ—éœ€è¦å…ˆ splitï¼Œå…¶å®ƒæ—§æ•°æ®é›†ç›´æ¥ ds[1]/ds[2]
        if 'credit' in name_l:
            ds, df_train, df_test = get_data_split(ds, seed=seed)
        else:
            df_train, df_test = ds[1], ds[2]

        target_column_name = ds[4][-1]
        dataset_description = ds[-1]
        return df_train, df_test, target_column_name, dataset_description

def base_model(seed):
    rforest = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight='balanced')
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)  # å¯é‡å¤çš„éšæœºæ•°æ®åˆ’åˆ†
    param_grid = {
        "min_samples_leaf": [0.001, 0.01, 0.05],  # è°ƒæ•´èŒƒå›´
        "max_depth": [5, 10, None]  # æ–°å¢æ·±åº¦æ§åˆ¶
    }
    gsmodel = GridSearchCV(rforest, param_grid, cv=cv, scoring='f1')

    return gsmodel

def code_exec(code):
    try:
        # å°è¯•ç¼–è¯‘æ£€æŸ¥ï¼ˆcompile æˆ AST å†æ‰§è¡Œï¼‰
        compiled_code = compile(code, "<string>", "exec")
        exec(compiled_code, globals())
        return None
    except Exception as e:
        print("Code could not be executed:", e)
        return str(e)

def print_stats(name, values):
    print(f"{name}: {np.mean(values):.2f} Â± {np.std(values):.2f}")

def clean_llm_code(code: str) -> str:
    import re
    # å»é™¤ ``` å¼€å¤´çš„ä»£ç å—æ ‡è®°å’Œæœ«å°¾é™„åŠ å†…å®¹
    code = re.sub(r"^```python\s*", "", code.strip(), flags=re.IGNORECASE)
    code = re.sub(r"```$", "", code.strip())

    # æ¸…é™¤ <end> å’Œéä»£ç æ–‡å­—ï¼ˆå¯èƒ½æ¥è‡ª LLMï¼‰
    code = re.sub(r"<end>", "", code)

    # ç§»é™¤ LLM è¾“å‡ºä¸­çš„è§£é‡Šæ®µæˆ–æ–‡æœ¬å¼€å¤´é”™è¯¯æç¤º
    lines = code.strip().splitlines()
    cleaned_lines = []
    for line in lines:
        if line.strip().startswith("class myclassifier") or line.strip().startswith("import") or line.strip().startswith(
                "from"):
            cleaned_lines.append(line)
        elif cleaned_lines:  # å¦‚æœå·²å¼€å§‹è®°å½•ä»£ç å—ï¼Œç»§ç»­æ·»åŠ åç»­ä»£ç 
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines)

def to_pd(df_train, target_name):
    y = df_train[target_name].astype(int)
    x = df_train.drop(target_name, axis=1)

    return x, y


if __name__ == '__main__':
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument('-g', '--gpus', default="0", type=str, help='GPUè®¾ç½®')
    parser.add_argument('-s', '--default_seed', default=42, type=int, help='éšæœºç§å­')
    parser.add_argument('-l', '--llm', default='gpt-3.5-turbo', type=str, help='å¤§æ¨¡å‹')
    # parser.add_argument('-l', '--llm', default='gpt-4o', type=str, help='å¤§æ¨¡å‹')
    parser.add_argument('-e', '--exam_iterations', default=5, type=int, help='å®éªŒæ¬¡æ•°')
    # parser.add_argument('-f', '--feat_iterations', default=1, type=int, help='ç‰¹å¾è¿­ä»£æ¬¡æ•°')
    parser.add_argument('-m', '--model_iterations', default=2, type=int, help='æ¨¡å‹è¿­ä»£æ¬¡æ•°')
    parser.add_argument('-p', '--param_iterations', default=2, type=int,help='å‚æ•°è°ƒä¼˜æ¬¡æ•°')
    args = parser.parse_args()

    """
    openAI API è®¾ç½® 
    """
    # TODO æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ API åœ°å€å’Œ Key
    base_url = '' # API åœ°å€
    api_key = '' # API Key

    # æ¨¡å‹æ ‡ç­¾ï¼Œå…¨å±€åŒºåˆ† LLM ç”Ÿæˆçš„æ¨¡å‹
    model_tab = 1

    for ds_name in ['ds_credit']:
        # cd1 cc1 ld1  cc2 cd2 cf1 balance-scale ds_credit
        print(f"=========== Dataset {ds_name} ===========")

        # ç”¨äºå­˜å‚¨æ¯æ¬¡é›†æˆå­¦ä¹ çš„æŒ‡æ ‡çš„ç»“æœ
        test_acc_list_ensemble = []
        test_f1_list_ensemble = []
        test_auc_list_ensemble = []
        test_pre_list_ensemble = []
        test_rec_list_ensemble = []

        # æ–°å¢ï¼šç”¨äºå­˜å‚¨VotingæŒ‡æ ‡çš„ç»“æœ
        test_acc_list_ensemble_voting = []
        test_f1_list_ensemble_voting = []
        test_auc_list_ensemble_voting = []
        test_pre_list_ensemble_voting = []
        test_rec_list_ensemble_voting = []

        # å®éªŒæ¬¡æ•°
        for exp in range(args.exam_iterations):
            print(f"=========== Experiment {exp + 1}/{args.exam_iterations} ===========")
            # å­˜å‚¨æ¯æ¬¡å®éªŒç»“æœçš„åˆ—è¡¨
            test_auc_list = []
            seed = args.default_seed + exp
            # è®¾ç½®éšæœºç§å­
            random.seed(seed)
            np.random.seed(seed)
            # åŠ è½½æ•°æ®é›†ã€åŠ è½½æ¨¡å‹
            df_train_aug, df_test_aug, target_column_name, dataset_description = load_origin_data(ds_name)
            df_train_aug,df_valid_aug = train_test_split(df_train_aug,test_size=0.25,random_state=seed,stratify=df_train_aug[target_column_name])
            baseline_model = base_model(seed)  # éšæœºæ£®æ— model

            # ç”Ÿæˆç‰¹å¾ å¾—åˆ°æ•°æ®å¢å¼ºåçš„è®­ç»ƒå’Œæµ‹è¯•é›†
            # print("ç‰¹å¾ç”Ÿæˆä¸­...")
            # df_train_aug, df_test_aug = generate_feat(
            #     base_classifier=baseline_model,  # è¯„ä»·ç”Ÿæˆç‰¹å¾çš„æ¨¡å‹æ˜¯ éšæœºæ£®æ—
            #     df_train=df_train_aug,
            #     df_test=df_test_aug,
            #     dataset_name=ds_name,
            #     round_num=exp + 1,
            #     llm_model=args.llm,
            #     iterations=args.feat_iterations,
            #     target_column_name=target_column_name,
            #     dataset_description=dataset_description
            # )
            # print("ç‰¹å¾ç”Ÿæˆå®Œæˆ")

            # æ•°æ®è½¬æ¢ å¾—åˆ°ç‰¹å¾çŸ©é˜µå’Œ æ ‡ç­¾ï¼ˆç›®æ ‡ï¼‰ å‘é‡
            train_aug_x, train_aug_y = to_pd(df_train_aug, target_column_name)
            val_aug_x, val_aug_y = to_pd(df_valid_aug, target_column_name)
            test_aug_x, test_aug_y = to_pd(df_test_aug, target_column_name)

            # æ„é€ æç¤ºè¯éœ€è¦çš„æ•°æ®æ ¼å¼
            s = build_prompt_samples(df_train_aug)

            # LLM ç”Ÿæˆ åˆ†ç±»æ¨¡å‹ä»£ç çš„æç¤ºè¯ prompt
            model_prompt = get_model_prompt(
                target_column_name=target_column_name,
                samples=s,
            )

            # LLM ç”Ÿæˆ åˆ†ç±»å™¨æ¨¡å‹ æç¤ºè¯
            model_messages = [
                {
                    "role": "system",
                    "content": (
                        "You are a top-level machine learning classification expert.\n"
                        "Your task is to help me iteratively search for the most suitable classifier model.\n"
                        "Your primary goal is to maximize the AUC (Area Under the ROC Curve) on the test set.\n"
                        "You must focus on improving AUC more than any other metric.\n"
                        "Your answer should only generate valid Python code."
                    ),
                },
                {
                    "role": "user",
                    "content": model_prompt,  # ä¿æŒç”Ÿæˆæ¨¡å‹çš„å…·ä½“æç¤ºç»“æ„ä¸å˜
                },
            ]


            # # å°† messages å†™å…¥æ–‡ä»¶ï¼Œä¾¿äºè°ƒè¯•å’ŒæŸ¥çœ‹
            # file_path = project_root + '/new_ensemble/classification_stacking/prompt.txt'
            # import json

            # with open(file_path, 'a') as f:
            #     # å°†æ¶ˆæ¯è½¬ä¸º JSON å­—ç¬¦ä¸²ï¼Œç¡®ä¿å¯è¯»æ€§ä¸”é€‚åˆå­˜å‚¨
            #     f.write(json.dumps(model_messages, ensure_ascii=False) + '\n')

            model_iter = args.model_iterations
            best_auc = 0
            best_code = None
            i = 0

            # å‚ä¸é›†æˆå­¦ä¹ åŸºç¡€æ¨¡å‹åˆ—è¡¨
            base_models = []
            # æ¨¡å‹ç”Ÿæˆè¿­ä»£
            while i < model_iter:
                try:
                    # ç”Ÿæˆä¸‹æ¸¸æ¨¡å‹ä»£ç 
                    code = generate_model(args.llm, model_messages,base_url,api_key)
                    # todo åŠ  code_clean ä»£ç 
                    code = clean_llm_code(code)

                    # åŠ¨æ€ä¿®æ”¹ç±»å
                    new_class_name = f"myclassifier_{i + 1}"
                    code = re.sub(r'class\s+myclassifier\w*\s*:', f'class {new_class_name}:', code)
                    print(f"----------------------------åŸå§‹ä»£ç -----------------------")
                    print(code)
                except Exception as e:
                    print("Error in LLM API." + str(e))
                    continue

                e = code_exec(code)
                # æ£€æŸ¥ç¼–è¯‘é”™è¯¯
                if e is not None:  # ç”Ÿæˆçš„ä»£ç æ‰§è¡Œå‡ºé”™ å°†é”™è¯¯ä¿¡æ¯åé¦ˆç»™LLMä»¥ç”Ÿæˆä¿®å¤åçš„ä»£ç 
                    model_messages += [
                        {"role": "assistant", "content": code},
                        {
                            "role": "user",
                            "content": f"""
                                The classifier code execution failed with error: {type(e)} {e}.\n Code: ```python{code}```
                                Remember, your answer should only generate code.
                                Do not include explanations or comments outside the code block.
                                Generate next code block(fixing error?):
                                """,
                        },
                    ]
                    continue


                try:
                    # æ¨¡å‹å®ä¾‹
                    model_class = globals()[new_class_name]
                    model = model_class()
                    model_copy = copy.deepcopy(model)
                    model.fit(train_aug_x, train_aug_y)
                    pred = model.predict(val_aug_x)
                    proba = model.predict_proba(val_aug_x)
                    model_list_append = model_copy

                except Exception as e:
                    print("Model code execution failed with error:" + str(e))
                    model_messages += [
                        {"role": "assistant", "content": code},
                        {
                            "role": "user",
                            "content": f"""
                            Code execution failed with error: {type(e)} {e}.\n Code: ```python{code}```\n Generate next code block(fixing error?):
                            """,
                        },
                    ]
                    continue


                test_auc = roc_auc_score(val_aug_y, proba) * 100

                # todo åœ¨è¿™åé¢åŠ å…¥å‚æ•°ä¼˜åŒ–
                print(f"----------------å‚æ•°ä¼˜åŒ–å¼€å§‹ï¼Œä¼˜åŒ–ä»£ç  {new_class_name}------------------")
                param_best_code = code
                param_best_auc = test_auc
                param_prompt = get_classification_param_prompt(
                    best_code=param_best_code,
                    best_auc=param_best_auc,
                    dataset_description=dataset_description,
                    X_test=val_aug_x,
                    feature_columns=train_aug_x.columns.tolist(),
                    dataset_name=ds_name,
                    max_rows=10
                )

                param_messages = [
                    {
                        "role": "system",
                        "content": "You are a classification optimization assistant.\n"
                                   "Your task is to help me improve the test AUC of the given classifier\n"
                                   "by tuning hyperparameters only. Your answer must contain only executable Python code."
                    },
                    {
                        "role": "user",
                        "content": param_prompt
                    },
                ]

                # å¼€å§‹å‚æ•°ä¼˜åŒ–è¿­ä»£
                for p_iter in range(args.param_iterations):
                    print(f"++++++ ç¬¬ {p_iter + 1} æ¬¡ä¼˜åŒ– +++++++")
                    try:
                        param_code = generate_model(args.llm, param_messages,base_url,api_key)
                        param_code = clean_llm_code(param_code)
                        param_new_class_name = f"myclassifier_{i+1}_param_{p_iter + 1}"
                        param_code = param_code.replace(f"class myclassifier_{i+1}:", f"class {param_new_class_name}:")
                        param_code = param_code.replace(f"class myclassifier_{i+1}_param_{p_iter}:",f"class {param_new_class_name}:")

                        param_err = code_exec(param_code)

                        if param_err is not None:
                            param_messages += [
                                {"role": "assistant", "content": param_code},
                                {"role": "user", "content": "Code failed. Fix it and regenerate."},
                            ]
                            continue

                        print('---------------ä¼˜åŒ–åçš„ä»£ç \n' + param_code)
                        # ä½¿ç”¨æ–°çš„ç±»ååˆ›å»ºæ¨¡å‹
                        model_class = globals()[param_new_class_name]
                        myclassifier_tuned = model_class()
                        model_copy = copy.deepcopy(myclassifier_tuned)
                        myclassifier_tuned.fit(train_aug_x, train_aug_y)
                        proba_tuned = myclassifier_tuned.predict_proba(val_aug_x)
                        auc_tuned = roc_auc_score(val_aug_y, proba_tuned) * 100

                        if auc_tuned > param_best_auc:
                            print(f"å‚æ•°ä¼˜åŒ–æ•ˆæœæå‡ï¼š{param_best_auc} --> {auc_tuned}")
                            param_best_auc = auc_tuned
                            param_best_code = param_code
                            model_list_append = model_copy


                        param_messages += [
                            {"role": "assistant", "content": param_code},
                            {"role": "user",
                             "content": f"Current AUC: {auc_tuned:.2f}, Best AUC: {param_best_auc:.2f}. Please improve further."},
                        ]

                    except Exception as e:
                        print("Tuning failed:", str(e))
                        continue

                # æ›´æ–°å…¨å±€æœ€ä½³æ¨¡å‹å’Œå…¨å±€æœ€ä½³å‚æ•°
                if best_auc < param_best_auc:
                    best_auc = param_best_auc
                    best_code = param_best_code

                # åŠ å…¥è°ƒå‚åæ¨¡å‹
                base_models.append(model_list_append)

                # å­˜å‚¨ç»“æœ
                test_auc_list.append(param_best_auc)

                # æ‰“å°å½“å‰å®éªŒè¯¦ç»†ç»“æœ
                print(f"å½“å‰å®éªŒç»“æœç¬¬ {i+1}/{args.model_iterations}")
                print(f"Test  AUC: {param_best_auc:.2f}")
                # while å¾ªç¯ç»§ç»­
                i = i + 1

                # ä¸‹ä¸€è½®æ¨¡å‹ç”Ÿæˆæç¤ºè¯æ‹¼æ¥
                if len(code) > 10:
                    model_messages += [
                        {"role": "assistant", "content": param_best_code},
                        {
                            "role": "user",
                            "content": f"""
                            âœ… The classifier code executed successfully.

                            ğŸ“ˆ Current model AUC: {param_best_auc:.4f}
                            ğŸ† Best historical AUC so far: {best_auc:.4f}

                            Please now propose a new classifier that is **more likely to improve the AUC** on the given test data.
                            The model must differ from all previous ones **by model type or internal structure**.

                            âš ï¸ Remember:
                            - You must only output valid Python code for a complete classifier named `myclassifier`.
                            - The class must include all imports and implement: `fit`, `predict`, and `predict_proba`.
                            - Do not repeat models you've already used.
                            - Prioritize models that provide reliable probabilistic outputs to help improve AUC.

                            ğŸ¯ Next code block:
                            """,
                        },
                    ]


            """
            é›†æˆå­¦ä¹   è°ƒç”¨  Stacking and Voting æ–¹æ³•
            """
            # æƒ³è¦æµ‹è¯•çš„å…ƒæ¨¡å‹åå­—åˆ—è¡¨
            metaModelName_list = [
                # 'RandomForestClassifier',
                # 'XGBClassifier',
                # 'LGBMClassifier',
                # 'CatBoostClassifier',
                # 'SVC',
                # 'DecisionTreeClassifier',
                'LogisticRegression',
                # 'BaggingClassifier',
            ]
            # æƒ³è¦æµ‹è¯•çš„å…ƒæ¨¡å‹åˆ—è¡¨
            metaModelName_list = getMetaModel_list(metaModelName_list)

            for meta_model in metaModelName_list:
                # è°ƒç”¨é›†æˆå­¦ä¹ æ–¹æ³•
                result = stacking_ensemble(base_models, meta_model,train_aug_x,train_aug_y,test_aug_x,test_aug_y)
                stacking_metrics = result['stacking_metrics']

                print(f"\n=========== ç¬¬{exp + 1}æ¬¡Stackingé›†æˆç»“æœ--{type(meta_model).__name__} ===========")
                print(f"Accuracy : {stacking_metrics['accuracy']:.4f}")
                print(f"F1 Score : {stacking_metrics['f1']:.4f}")
                print(f"AUC      : {stacking_metrics['auc']:.4f}")
                print(f"Precision: {stacking_metrics['precision']:.4f}")
                print(f"Recall   : {stacking_metrics['recall']:.4f}")
                # ä¿å­˜é›†æˆç»“æœ
                test_acc_list_ensemble.append(round(stacking_metrics['accuracy'], 5)*100)
                test_f1_list_ensemble.append(round(stacking_metrics['f1'], 5)*100)
                test_pre_list_ensemble.append(round(stacking_metrics['precision'], 5)*100)
                test_rec_list_ensemble.append(round(stacking_metrics['recall'], 5)*100)
                test_auc_list_ensemble.append(round(stacking_metrics['auc'], 5)*100)

                
            """
            æ–°å¢:Voting é›†æˆ
            ç›´æ¥å¯¹ä¸æœ¬è½®ç”Ÿæˆ/è°ƒå‚åçš„ base_models è¿›è¡ŒæŠ•ç¥¨èåˆï¼Œå¹¶è¯„ä¼°æŒ‡æ ‡
            """
            voting_result = voting_ensemble(base_models, test_aug_x, test_aug_y)
            voting_metrics = voting_result['metrics']

            print(f"\n=========== ç¬¬{exp + 1}æ¬¡ Voting é›†æˆç»“æœ ===========")
            print(f"Accuracy : {voting_metrics['accuracy']:.4f}")
            print(f"F1 Score : {voting_metrics['f1']:.4f}")
            print(f"AUC      : {voting_metrics['auc']:.4f}")
            print(f"Precision: {voting_metrics['precision']:.4f}")
            print(f"Recall   : {voting_metrics['recall']:.4f}")

            # ä¿å­˜Votingé›†æˆç»“æœ
            test_acc_list_ensemble_voting.append(round(voting_metrics['accuracy'], 5)*100)
            test_f1_list_ensemble_voting.append(round(voting_metrics['f1'], 5)*100)
            test_pre_list_ensemble_voting.append(round(voting_metrics['precision'], 5)*100)
            test_rec_list_ensemble_voting.append(round(voting_metrics['recall'], 5)*100)
            test_auc_list_ensemble_voting.append(round(voting_metrics['auc'], 5)*100)

        # å®éªŒè¿­ä»£ç»“æŸï¼Œç»Ÿè®¡é›†æˆå­¦ä¹ çš„ç»“æœ
        print(f"\n=========== å®éªŒç»“æŸï¼Œå…±{args.exam_iterations}æ¬¡é›†æˆå­¦ä¹ ç»Ÿè®¡ç»“æœä¿¡æ¯ ===========")
        print("\nStacking é›†æˆå­¦ä¹ ç»“æœ:")
        print('acc: ' + format_mean_std(test_acc_list_ensemble))
        print('f1: ' + format_mean_std(test_f1_list_ensemble))
        print('auc: ' + format_mean_std(test_auc_list_ensemble))
        print('pre: ' + format_mean_std(test_pre_list_ensemble))
        print('rec: ' + format_mean_std(test_rec_list_ensemble))
        print("\nVoting é›†æˆå­¦ä¹ ç»“æœ:")
        print('voting_acc: ' + format_mean_std(test_acc_list_ensemble_voting))
        print('voting_f1: ' + format_mean_std(test_f1_list_ensemble_voting))
        print('voting_auc: ' + format_mean_std(test_auc_list_ensemble_voting))
        print('voting_pre: ' + format_mean_std(test_pre_list_ensemble_voting))
        print('voting_rec: ' + format_mean_std(test_rec_list_ensemble_voting))